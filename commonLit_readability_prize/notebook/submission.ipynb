{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0991e22e-1402-4078-b026-47507c6ccbce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 4 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from typing import AnyStr, List, Optional\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import textstat\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from pandarallel import pandarallel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    get_cosine_schedule_with_warmup,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "pandarallel.initialize(progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d088d8-4a7c-4054-9fc5-5ea1fc004cf5",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82728231-c6bb-44a6-a555-e60866e13b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessed_excerpt(src_data: pd.DataFrame) -> pd.DataFrame:\n",
    "    def preprocess_excerpt(text: AnyStr):\n",
    "        text = re.sub(\"[^a-zA-Z]\", \" \", text).lower()\n",
    "        text = nltk.word_tokenize(text)  # NOTE: 英文を単語分割する\n",
    "        text = [word for word in text if word not in set(stopwords.words(\"english\"))]\n",
    "\n",
    "        lemma = nltk.WordNetLemmatizer()  # NOTE: 複数形の単語を単数形に変換する\n",
    "        text = \" \".join([lemma.lemmatize(word) for word in text])\n",
    "        return text\n",
    "\n",
    "    dst_data = src_data[\"excerpt\"].parallel_apply(preprocess_excerpt)\n",
    "    return dst_data\n",
    "\n",
    "\n",
    "def get_textstat(src_data: pd.DataFrame) -> pd.DataFrame:\n",
    "    dst_data = pd.DataFrame()\n",
    "\n",
    "    dst_data = dst_data.assign(\n",
    "        excerpt_len=src_data[\"preprocessed_excerpt\"].str.len(),\n",
    "        avg_word_len=(\n",
    "            src_data[\"preprocessed_excerpt\"]\n",
    "            .apply(lambda x: [len(s) for s in x.split()])\n",
    "            .map(np.mean)\n",
    "        ),\n",
    "        char_count=src_data[\"excerpt\"].map(textstat.char_count),\n",
    "        word_count=src_data[\"preprocessed_excerpt\"].map(textstat.lexicon_count),\n",
    "        sentence_count=src_data[\"excerpt\"].map(textstat.sentence_count),\n",
    "        syllable_count=src_data[\"excerpt\"].apply(textstat.syllable_count),\n",
    "        smog_index=src_data[\"excerpt\"].apply(textstat.smog_index),\n",
    "        automated_readability_index=src_data[\"excerpt\"].apply(\n",
    "            textstat.automated_readability_index\n",
    "        ),\n",
    "        coleman_liau_index=src_data[\"excerpt\"].apply(textstat.coleman_liau_index),\n",
    "        linsear_write_formula=src_data[\"excerpt\"].apply(textstat.linsear_write_formula),\n",
    "    )\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    feat_cols = dst_data.columns.tolist()\n",
    "    dst_data[feat_cols] = scaler.fit_transform(dst_data)\n",
    "    return dst_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30219219-17c1-49e0-84d6-e3a271273fb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d1bf4b72bdf41e69dd58ae43717ca48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=2), Label(value='0 / 2'))), HBox(c…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = pd.read_csv(\"../data/raw/test.csv\", usecols=[\"id\", \"excerpt\"])\n",
    "test[\"preprocessed_excerpt\"] = get_preprocessed_excerpt(test)\n",
    "textstat_feat = get_textstat(test)\n",
    "\n",
    "test = pd.concat([test, textstat_feat], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333cc86b-b263-47ab-902c-7254775c5c9f",
   "metadata": {},
   "source": [
    "## Predict with RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a749560f-5db3-434d-a783-0783e2696cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, yhat, y):\n",
    "        loss = torch.sqrt(self.mse(yhat, y) + self.eps)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dbe1061a-7055-48d1-ade9-92068e89d529",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommonLitDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        tokenizer: transformers.PreTrainedTokenizer,\n",
    "        max_len: int = 256,\n",
    "        is_test: int = False,\n",
    "    ):\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "        self.excerpt = data[[\"excerpt\"]].to_numpy()\n",
    "\n",
    "        if is_test:\n",
    "            self.target = np.zeros((len(data), 1))\n",
    "            self.textstat = np.zeros((len(data), 1))\n",
    "        else:\n",
    "            self.target = data[[\"target\"]].to_numpy()\n",
    "            textstat = data.drop([\"excerpt\", \"target\"], axis=1)\n",
    "            self.textstat = textstat.to_numpy()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.excerpt)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.excerpt[idx])\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=True,\n",
    "        )\n",
    "        textstat = self.textstat[idx]\n",
    "        target = self.target[idx]\n",
    "\n",
    "        return {\n",
    "            \"inputs\": {\n",
    "                \"input_ids\": torch.tensor(inputs[\"input_ids\"], dtype=torch.long),\n",
    "                \"attention_mask\": torch.tensor(\n",
    "                    inputs[\"attention_mask\"], dtype=torch.long\n",
    "                ),\n",
    "                \"token_type_ids\": torch.tensor(\n",
    "                    inputs[\"token_type_ids\"], dtype=torch.long\n",
    "                ),\n",
    "            },\n",
    "            \"textstat\": torch.tensor(textstat, dtype=torch.float32),\n",
    "            \"target\": torch.tensor(target, dtype=torch.float32),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2955b586-0572-49ab-8e76-c2a17a4be458",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommonLitRoBERTaModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name_or_path: str = \"roberta-base\",\n",
    "        output_hidden_states: bool = False,\n",
    "    ):\n",
    "        super(CommonLitRoBERTaModel, self).__init__()\n",
    "        self.roberta = AutoModel.from_pretrained(\n",
    "            model_name_or_path,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "        )\n",
    "        self.config = self.roberta.config\n",
    "\n",
    "        reg_input_dim = 768\n",
    "        self.regression_head = nn.Sequential(\n",
    "            nn.LayerNorm(reg_input_dim),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(reg_input_dim, 1),\n",
    "        )\n",
    "        # Initialize Weights\n",
    "        self.regression_head.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        outputs = self.roberta(**batch[\"inputs\"])\n",
    "        pooler_output = outputs.pooler_output\n",
    "\n",
    "        # NOTE: この段階でtextstat特徴量を追加すると、fine-tuning時の学習率と相性が悪く過学習する可能性が高い、\n",
    "        # x = torch.cat((pooler_output, batch[\"textstat\"]), dim=1)\n",
    "        x = self.regression_head(pooler_output)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CommonLitModel(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        lr: float = 5e-5,\n",
    "        num_epoch: int = 10,\n",
    "        roberta_model_name_or_path: str = \"roberta-base\",\n",
    "        output_hidden_states: bool = False,\n",
    "        lr_scheduler: str = \"linear\",\n",
    "        lr_interval: str = \"epoch\",\n",
    "        lr_warmup_step: int = 0,\n",
    "        lr_num_cycles: int = 0.5,\n",
    "    ):\n",
    "        super(CommonLitModel, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.roberta_model = CommonLitRoBERTaModel(\n",
    "            model_name_or_path=roberta_model_name_or_path,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "        )\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.eval_fn = RMSELoss()\n",
    "\n",
    "    def forward(self, batch):\n",
    "        z = self.roberta_model(batch)\n",
    "        return z\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer_grouped_parameters = self._get_optimizer_params(self.roberta_model)\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            optimizer_grouped_parameters,  # self.parameters()\n",
    "            lr=self.hparams.lr,\n",
    "            betas=(0.9, 0.999),\n",
    "            eps=1e-8,\n",
    "            weight_decay=0,\n",
    "        )\n",
    "\n",
    "        total_steps = self.train_dataloader_len * self.hparams.num_epoch\n",
    "        if self.hparams.lr_scheduler == \"linear\":\n",
    "            # Linear scheduler\n",
    "            lr_scheduler = get_linear_schedule_with_warmup(\n",
    "                optimizer,\n",
    "                num_warmup_steps=0,\n",
    "                num_training_steps=total_steps,\n",
    "            )\n",
    "        elif self.hparams.lr_scheduler == \"cosine\":\n",
    "            # Cosine scheduler\n",
    "            lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "                optimizer,\n",
    "                num_warmup_steps=self.hparams.lr_warmup_step,\n",
    "                num_training_steps=total_steps,\n",
    "                num_cycles=self.hparams.lr_num_cycles,\n",
    "            )\n",
    "        else:\n",
    "            # Linear scheduler\n",
    "            lr_scheduler = get_linear_schedule_with_warmup(\n",
    "                optimizer,\n",
    "                num_warmup_steps=self.hparams.lr_warmup_step,\n",
    "                num_training_steps=total_steps,\n",
    "            )\n",
    "\n",
    "        lr_dict = {\n",
    "            \"scheduler\": lr_scheduler,\n",
    "            \"interval\": self.hparams.lr_interval,  # step or epoch\n",
    "            \"strict\": True,\n",
    "        }\n",
    "\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": lr_dict}\n",
    "\n",
    "    def _get_optimizer_params(self, model):\n",
    "        # Differential learning rate and weight decay\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        learning_rate = self.hparams.lr\n",
    "        no_decay = [\"bias\", \"gamma\", \"beta\"]\n",
    "        group1 = [\"layer.0.\", \"layer.1.\", \"layer.2.\", \"layer.3.\"]\n",
    "        group2 = [\"layer.4.\", \"layer.5.\", \"layer.6.\", \"layer.7.\"]\n",
    "        group3 = [\"layer.8.\", \"layer.9.\", \"layer.10.\", \"layer.11.\"]\n",
    "        group_all = [\n",
    "            \"layer.0.\",\n",
    "            \"layer.1.\",\n",
    "            \"layer.2.\",\n",
    "            \"layer.3.\",\n",
    "            \"layer.4.\",\n",
    "            \"layer.5.\",\n",
    "            \"layer.6.\",\n",
    "            \"layer.7.\",\n",
    "            \"layer.8.\",\n",
    "            \"layer.9.\",\n",
    "            \"layer.10.\",\n",
    "            \"layer.11.\",\n",
    "        ]\n",
    "        optimizer_parameters = [\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p\n",
    "                    for n, p in model.roberta.named_parameters()\n",
    "                    if not any(nd in n for nd in no_decay)\n",
    "                    and not any(nd in n for nd in group_all)\n",
    "                ],\n",
    "                \"weight_decay_rate\": 0.01,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p\n",
    "                    for n, p in model.roberta.named_parameters()\n",
    "                    if not any(nd in n for nd in no_decay)\n",
    "                    and any(nd in n for nd in group1)\n",
    "                ],\n",
    "                \"weight_decay_rate\": 0.01,\n",
    "                \"lr\": learning_rate / 2.6,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p\n",
    "                    for n, p in model.roberta.named_parameters()\n",
    "                    if not any(nd in n for nd in no_decay)\n",
    "                    and any(nd in n for nd in group2)\n",
    "                ],\n",
    "                \"weight_decay_rate\": 0.01,\n",
    "                \"lr\": learning_rate,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p\n",
    "                    for n, p in model.roberta.named_parameters()\n",
    "                    if not any(nd in n for nd in no_decay)\n",
    "                    and any(nd in n for nd in group3)\n",
    "                ],\n",
    "                \"weight_decay_rate\": 0.01,\n",
    "                \"lr\": learning_rate * 2.6,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p\n",
    "                    for n, p in model.roberta.named_parameters()\n",
    "                    if any(nd in n for nd in no_decay)\n",
    "                    and not any(nd in n for nd in group_all)\n",
    "                ],\n",
    "                \"weight_decay_rate\": 0.0,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p\n",
    "                    for n, p in model.roberta.named_parameters()\n",
    "                    if any(nd in n for nd in no_decay) and any(nd in n for nd in group1)\n",
    "                ],\n",
    "                \"weight_decay_rate\": 0.0,\n",
    "                \"lr\": learning_rate / 2.6,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p\n",
    "                    for n, p in model.roberta.named_parameters()\n",
    "                    if any(nd in n for nd in no_decay) and any(nd in n for nd in group2)\n",
    "                ],\n",
    "                \"weight_decay_rate\": 0.0,\n",
    "                \"lr\": learning_rate,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p\n",
    "                    for n, p in model.roberta.named_parameters()\n",
    "                    if any(nd in n for nd in no_decay) and any(nd in n for nd in group3)\n",
    "                ],\n",
    "                \"weight_decay_rate\": 0.0,\n",
    "                \"lr\": learning_rate * 2.6,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p for n, p in model.named_parameters() if \"roberta\" not in n\n",
    "                ],\n",
    "                \"lr\": 1e-3,  # learning_rate\n",
    "                \"momentum\": 0.99,\n",
    "            },\n",
    "        ]\n",
    "        return optimizer_parameters\n",
    "\n",
    "    def shared_step(self, batch):\n",
    "        z = self(batch)\n",
    "        return z\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        z = self.shared_step(batch)\n",
    "        loss = self.loss_fn(z, batch[\"target\"])\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        z = self.shared_step(batch)\n",
    "        return {\"pred\": z, \"target\": batch[\"target\"]}\n",
    "\n",
    "    def validation_step_end(self, batch_parts):\n",
    "        return batch_parts\n",
    "\n",
    "    def validation_epoch_end(self, validation_step_outputs):\n",
    "        pred = []\n",
    "        target = []\n",
    "\n",
    "        for output in validation_step_outputs:\n",
    "            pred.append(output[\"pred\"])\n",
    "            target.append(output[\"target\"])\n",
    "\n",
    "        pred = torch.cat(pred, dim=0)\n",
    "        target = torch.cat(target, dim=0)\n",
    "\n",
    "        loss = self.loss_fn(pred, target)\n",
    "        metric = self.eval_fn(pred, target)\n",
    "\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_metric\", metric, prog_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "acb08af5-621b-4be5-b130-caa6d240a823",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(data: pd.DataFrame):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "    dataset = CommonLitDataset(data, tokenizer, 256, is_test=True)\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=32,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f234e597-18d3-4955-888b-b0bc8be5034c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_by_ckpt(data, checkpoints):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    dataloader = get_dataloader(data)\n",
    "\n",
    "    pred = []\n",
    "    for _, ckpt in enumerate(checkpoints):\n",
    "        print(f\"Predicted by {ckpt}\")\n",
    "\n",
    "        model = CommonLitModel().load_from_checkpoint(ckpt)\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        model.freeze()\n",
    "\n",
    "        pred_ckpt = []\n",
    "        for batch in dataloader:\n",
    "            batch[\"inputs\"][\"input_ids\"] = batch[\"inputs\"][\"input_ids\"].to(device)\n",
    "            batch[\"inputs\"][\"attention_mask\"] = batch[\"inputs\"][\"attention_mask\"].to(\n",
    "                device\n",
    "            )\n",
    "            batch[\"inputs\"][\"token_type_ids\"] = batch[\"inputs\"][\"token_type_ids\"].to(\n",
    "                device\n",
    "            )\n",
    "            batch[\"textstat\"] = batch[\"textstat\"].to(device)\n",
    "\n",
    "            z = model(batch)\n",
    "            pred_ckpt.append(z)\n",
    "\n",
    "        pred_ckpt = torch.cat(pred_ckpt, dim=0).detach().cpu().numpy().copy()\n",
    "        pred.append(pred_ckpt)\n",
    "\n",
    "    return pred\n",
    "\n",
    "\n",
    "def get_ckpt_path(checkpoint_path: str) -> List:\n",
    "    with open(checkpoint_path, \"r\") as f:\n",
    "        txt = f.readlines()\n",
    "\n",
    "    model_version = \"RoBERTa-Baseline\"\n",
    "    dir_path = \"../data/models/roberta/\"\n",
    "    checkpoints = [t.strip() for t in txt]\n",
    "    checkpoints = [ckpt.replace(\"../tb_logs/\", dir_path) for ckpt in checkpoints]\n",
    "    return checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "637e6285-8ca0-4381-8e87-aa7039c41e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted by ../data/models/roberta/RoBERTa-Baseline/version_0/checkpoints/epoch=07-loss=0.0000-val_loss=0.2481-val_metric=0.4980.ckpt\n",
      "Predicted by ../data/models/roberta/RoBERTa-Baseline/version_1/checkpoints/epoch=13-loss=0.0000-val_loss=0.2724-val_metric=0.5219.ckpt\n",
      "Predicted by ../data/models/roberta/RoBERTa-Baseline/version_2/checkpoints/epoch=04-loss=0.0000-val_loss=0.2484-val_metric=0.4984.ckpt\n",
      "Predicted by ../data/models/roberta/RoBERTa-Baseline/version_3/checkpoints/epoch=05-loss=0.0000-val_loss=0.2018-val_metric=0.4492.ckpt\n",
      "Predicted by ../data/models/roberta/RoBERTa-Baseline/version_4/checkpoints/epoch=06-loss=0.0000-val_loss=0.2538-val_metric=0.5038.ckpt\n",
      "Predicted by ../data/models/roberta/RoBERTa-Baseline/version_5/checkpoints/epoch=05-loss=0.0000-val_loss=0.2604-val_metric=0.5103.ckpt\n",
      "Predicted by ../data/models/roberta/RoBERTa-Baseline/version_6/checkpoints/epoch=07-loss=0.0000-val_loss=0.2343-val_metric=0.4841.ckpt\n",
      "Predicted by ../data/models/roberta/RoBERTa-Baseline/version_7/checkpoints/epoch=05-loss=0.0000-val_loss=0.2434-val_metric=0.4933.ckpt\n",
      "Predicted by ../data/models/roberta/RoBERTa-Baseline/version_8/checkpoints/epoch=11-loss=0.0000-val_loss=0.2328-val_metric=0.4824.ckpt\n",
      "Predicted by ../data/models/roberta/RoBERTa-Baseline/version_9/checkpoints/epoch=09-loss=0.0000-val_loss=0.2574-val_metric=0.5073.ckpt\n",
      "Predicted by ../data/models/roberta/RoBERTa-Baseline/version_10/checkpoints/epoch=03-loss=0.0000-val_loss=0.2414-val_metric=0.4913.ckpt\n",
      "Predicted by ../data/models/roberta/RoBERTa-Baseline/version_11/checkpoints/epoch=08-loss=0.0000-val_loss=0.2465-val_metric=0.4965.ckpt\n",
      "Predicted by ../data/models/roberta/RoBERTa-Baseline/version_12/checkpoints/epoch=05-loss=0.0000-val_loss=0.2427-val_metric=0.4927.ckpt\n",
      "Predicted by ../data/models/roberta/RoBERTa-Baseline/version_13/checkpoints/epoch=11-loss=0.0000-val_loss=0.2593-val_metric=0.5092.ckpt\n",
      "Predicted by ../data/models/roberta/RoBERTa-Baseline/version_14/checkpoints/epoch=04-loss=0.0000-val_loss=0.2578-val_metric=0.5078.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Predict by RoBERTa\n",
    "ckpt_path = \"../data/models/roberta/best_checkpoints_0.496413±0.0162.txt\"\n",
    "checkpoints = get_ckpt_path(ckpt_path)\n",
    "\n",
    "pred = predict_by_ckpt(test, checkpoints)\n",
    "test[[f\"pred_{i}\" for i in range(len(checkpoints))]] = pred\n",
    "\n",
    "X_pred = test[[f\"pred_{i}\" for i in range(len(checkpoints))]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "57b00eab-441d-4479-b1e4-4228d57c9fb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_0</th>\n",
       "      <th>pred_1</th>\n",
       "      <th>pred_2</th>\n",
       "      <th>pred_3</th>\n",
       "      <th>pred_4</th>\n",
       "      <th>pred_5</th>\n",
       "      <th>pred_6</th>\n",
       "      <th>pred_7</th>\n",
       "      <th>pred_8</th>\n",
       "      <th>pred_9</th>\n",
       "      <th>pred_10</th>\n",
       "      <th>pred_11</th>\n",
       "      <th>pred_12</th>\n",
       "      <th>pred_13</th>\n",
       "      <th>pred_14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.504130</td>\n",
       "      <td>-0.473537</td>\n",
       "      <td>-0.540328</td>\n",
       "      <td>-0.432704</td>\n",
       "      <td>-0.379853</td>\n",
       "      <td>-0.250393</td>\n",
       "      <td>-0.230802</td>\n",
       "      <td>-0.461931</td>\n",
       "      <td>-0.320304</td>\n",
       "      <td>-0.396811</td>\n",
       "      <td>-0.434372</td>\n",
       "      <td>-0.266711</td>\n",
       "      <td>-0.336394</td>\n",
       "      <td>-0.188015</td>\n",
       "      <td>-0.262966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.359742</td>\n",
       "      <td>-0.384180</td>\n",
       "      <td>-0.450077</td>\n",
       "      <td>-0.375443</td>\n",
       "      <td>-0.402787</td>\n",
       "      <td>-0.301277</td>\n",
       "      <td>-0.091749</td>\n",
       "      <td>-0.088621</td>\n",
       "      <td>-0.240885</td>\n",
       "      <td>-0.199681</td>\n",
       "      <td>-0.426405</td>\n",
       "      <td>-0.484604</td>\n",
       "      <td>-0.402243</td>\n",
       "      <td>-0.385206</td>\n",
       "      <td>-0.597047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.139508</td>\n",
       "      <td>-0.456083</td>\n",
       "      <td>-0.470303</td>\n",
       "      <td>-0.428771</td>\n",
       "      <td>-0.389819</td>\n",
       "      <td>-0.459563</td>\n",
       "      <td>-0.416336</td>\n",
       "      <td>-0.433987</td>\n",
       "      <td>-0.196438</td>\n",
       "      <td>-0.353424</td>\n",
       "      <td>-0.610762</td>\n",
       "      <td>-0.673162</td>\n",
       "      <td>-0.588096</td>\n",
       "      <td>-0.411148</td>\n",
       "      <td>-0.418171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.508181</td>\n",
       "      <td>-2.309677</td>\n",
       "      <td>-2.228820</td>\n",
       "      <td>-2.345114</td>\n",
       "      <td>-2.402572</td>\n",
       "      <td>-2.391798</td>\n",
       "      <td>-2.559634</td>\n",
       "      <td>-2.266225</td>\n",
       "      <td>-2.113398</td>\n",
       "      <td>-2.698644</td>\n",
       "      <td>-2.476523</td>\n",
       "      <td>-2.241605</td>\n",
       "      <td>-2.594661</td>\n",
       "      <td>-2.761666</td>\n",
       "      <td>-2.130869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.999642</td>\n",
       "      <td>-1.796057</td>\n",
       "      <td>-1.763693</td>\n",
       "      <td>-1.621618</td>\n",
       "      <td>-1.979182</td>\n",
       "      <td>-1.580597</td>\n",
       "      <td>-1.572743</td>\n",
       "      <td>-1.535815</td>\n",
       "      <td>-1.527643</td>\n",
       "      <td>-1.720838</td>\n",
       "      <td>-2.076024</td>\n",
       "      <td>-1.467190</td>\n",
       "      <td>-1.642941</td>\n",
       "      <td>-1.741860</td>\n",
       "      <td>-1.400126</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     pred_0    pred_1    pred_2    pred_3    pred_4    pred_5    pred_6  \\\n",
       "0 -0.504130 -0.473537 -0.540328 -0.432704 -0.379853 -0.250393 -0.230802   \n",
       "1 -0.359742 -0.384180 -0.450077 -0.375443 -0.402787 -0.301277 -0.091749   \n",
       "2 -0.139508 -0.456083 -0.470303 -0.428771 -0.389819 -0.459563 -0.416336   \n",
       "3 -2.508181 -2.309677 -2.228820 -2.345114 -2.402572 -2.391798 -2.559634   \n",
       "4 -1.999642 -1.796057 -1.763693 -1.621618 -1.979182 -1.580597 -1.572743   \n",
       "\n",
       "     pred_7    pred_8    pred_9   pred_10   pred_11   pred_12   pred_13  \\\n",
       "0 -0.461931 -0.320304 -0.396811 -0.434372 -0.266711 -0.336394 -0.188015   \n",
       "1 -0.088621 -0.240885 -0.199681 -0.426405 -0.484604 -0.402243 -0.385206   \n",
       "2 -0.433987 -0.196438 -0.353424 -0.610762 -0.673162 -0.588096 -0.411148   \n",
       "3 -2.266225 -2.113398 -2.698644 -2.476523 -2.241605 -2.594661 -2.761666   \n",
       "4 -1.535815 -1.527643 -1.720838 -2.076024 -1.467190 -1.642941 -1.741860   \n",
       "\n",
       "    pred_14  \n",
       "0 -0.262966  \n",
       "1 -0.597047  \n",
       "2 -0.418171  \n",
       "3 -2.130869  \n",
       "4 -1.400126  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pred.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83b89db-7ca2-48ba-b3ae-36f63e1058ec",
   "metadata": {},
   "source": [
    "## Predict with SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9581efec-8e7c-487e-a80b-f693a6738350",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data: pd.DataFrame, model_dir: str, n_splits: int) -> np.ndarray:\n",
    "    pred = np.zeros(data.shape[0])\n",
    "    for n_fold in range(n_splits):\n",
    "        with open(os.path.join(model_dir, f\"{n_fold}-fold.pkl\"), mode=\"rb\") as file:\n",
    "            model = pickle.load(file)\n",
    "\n",
    "        pred += model.predict(data) / n_splits\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "87b85a21-3fc3-44e1-9c87-f6e52932bbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"../data/models/svr/\"\n",
    "# model_dir = \"../data/models/xgb/\"\n",
    "\n",
    "submission = test[[\"id\"]].copy()\n",
    "submission[\"target\"] = predict(X_pred, model_dir, 5)\n",
    "\n",
    "# submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3a37ad4d-55d5-422f-9485-47a07b596e1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c0f722661</td>\n",
       "      <td>-0.435540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f0953f0a5</td>\n",
       "      <td>-0.384058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0df072751</td>\n",
       "      <td>-0.482642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>04caf4e0c</td>\n",
       "      <td>-2.479009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0e63f8bea</td>\n",
       "      <td>-1.647921</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id    target\n",
       "0  c0f722661 -0.435540\n",
       "1  f0953f0a5 -0.384058\n",
       "2  0df072751 -0.482642\n",
       "3  04caf4e0c -2.479009\n",
       "4  0e63f8bea -1.647921"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
