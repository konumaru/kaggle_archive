{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0a2e2a3-75de-4717-962f-addee2792e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModel,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "157dca7b-b988-4461-a91f-63c45ce18c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitModel(nn.Module):\n",
    "    def __init__(self, model_name_or_path=\"roberta-base\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = AutoConfig.from_pretrained(model_name_or_path)\n",
    "        self.config.update({\n",
    "            \"output_hidden_states\":True, \n",
    "            \"hidden_dropout_prob\": 0.0,\n",
    "            \"layer_norm_eps\": 1e-7\n",
    "        })                       \n",
    "        \n",
    "        self.roberta = AutoModel.from_pretrained(model_name_or_path, config=self.config)  \n",
    "        \n",
    "        hidden_size = self.config.hidden_size\n",
    "        self.attention = nn.Sequential(            \n",
    "            nn.Linear(hidden_size, 512),            \n",
    "            nn.Tanh(),                       \n",
    "            nn.Linear(512, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )        \n",
    "\n",
    "        self.regressor = nn.Sequential(                        \n",
    "            nn.Linear(hidden_size, 1)                        \n",
    "        )\n",
    "\n",
    "        self._init_embed_layers(reinit_layers=4)\n",
    "\n",
    "    def _init_embed_layers(self, reinit_layers: int = 4):\n",
    "        if reinit_layers > 0:\n",
    "            for layer in self.roberta.encoder.layer[-reinit_layers:]:\n",
    "                for module in layer.modules():\n",
    "                    if isinstance(module, nn.Linear):\n",
    "                        module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "                        if module.bias is not None:\n",
    "                            module.bias.data.zero_()\n",
    "                    elif isinstance(module, nn.Embedding):\n",
    "                        module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "                        if module.padding_idx is not None:\n",
    "                            module.weight.data[module.padding_idx].zero_()\n",
    "                    elif isinstance(module, nn.LayerNorm):\n",
    "                        module.bias.data.zero_()\n",
    "                        module.weight.data.fill_(1.0)\n",
    "        \n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        roberta_output = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        last_layer_hidden_states = roberta_output.hidden_states[-1]\n",
    "        weights = self.attention(last_layer_hidden_states)\n",
    "        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)\n",
    "        # Now we reduce the context vector to the prediction score.\n",
    "        return self.regressor(context_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9b6439ce-4a71-43f8-a232-c185b786f40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LitModel(\"roberta-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8c2cf1d0-2985-403c-86b9-75386e34d967",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizer(model):\n",
    "    named_parameters = list(model.named_parameters())\n",
    "    \n",
    "    roberta_parameters = named_parameters[:197]    \n",
    "    attention_parameters = named_parameters[199:203]\n",
    "    regressor_parameters = named_parameters[203:]\n",
    "        \n",
    "    attention_group = [params for (name, params) in attention_parameters]\n",
    "    regressor_group = [params for (name, params) in regressor_parameters]\n",
    "\n",
    "    parameters = []\n",
    "    parameters.append({\"params\": attention_group})\n",
    "    parameters.append({\"params\": regressor_group})\n",
    "\n",
    "    for layer_num, (name, params) in enumerate(roberta_parameters):\n",
    "        weight_decay = 0.0 if \"bias\" in name else 0.01\n",
    "\n",
    "        lr = 2e-5\n",
    "\n",
    "        if layer_num >= 69:        \n",
    "            lr = 5e-5\n",
    "\n",
    "        if layer_num >= 133:\n",
    "            lr = 1e-4\n",
    "\n",
    "        parameters.append({\"params\": params, \"weight_decay\": weight_decay, \"lr\": lr})\n",
    "\n",
    "    return AdamW(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "90b3c46a-f4dc-457e-bd63-b1e7a50d3ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "past_result = create_optimizer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5be7e1fe-6592-4dfc-b423-cae860494e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizer(model):\n",
    "    named_parameters = list(model.named_parameters())    \n",
    "    roberta_parameters = [(n, p) for n, p in named_parameters if 'roberta' in n]\n",
    "    not_roberta_parameters = [(n, p) for n, p in named_parameters if 'roberta' not in n]\n",
    "\n",
    "    not_roberta_group = [p for n, p in not_roberta_parameters]\n",
    "\n",
    "    parameters = []\n",
    "    parameters.append({\"params\": not_roberta_group})\n",
    "\n",
    "    group_1 = [f\"layer.{i}\" for i in range(0, 5)]\n",
    "    group_2 = [f\"layer.{i}\" for i in range(5, 9)]\n",
    "    group_3 = [f\"layer.{i}\" for i in range(9, 12)]\n",
    "    for name, params in roberta_parameters:\n",
    "        weight_decay = 0.0 if \"bias\" in name else 0.01\n",
    "\n",
    "        if any([(g in name) for g in group_1]):\n",
    "            lr = 2e-5\n",
    "        elif any([(g in name) for g in group_2]):\n",
    "            lr = 5e-5\n",
    "        elif any([(g in name) for g in group_3]):\n",
    "            lr = 1e-4\n",
    "        else:\n",
    "            lr = 1e-4\n",
    "\n",
    "        parameters.append({\"params\": params, \"weight_decay\": weight_decay, \"lr\": lr})\n",
    "        \n",
    "    return AdamW(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "75fd2948-045f-4272-9529-35f3b0f75712",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_result = create_optimizer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca251b0-218f-4a57-97b0-0867e9d36fa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771f5d1c-d5a4-477d-bec5-90f4d7837173",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae17ef74-9d0a-425f-85fb-b8dbdd9638d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
