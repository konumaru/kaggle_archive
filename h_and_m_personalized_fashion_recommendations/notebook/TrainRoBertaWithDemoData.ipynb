{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a8944d7a-919c-4d57-9d4e-7815daa072f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "767eacf6-6d26-472a-aa3c-86be336bc04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_article_ids = 1000\n",
    "max_seq_len = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5774e18c-35a7-46d7-b503-72651700b97a",
   "metadata": {},
   "source": [
    "## Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "81318183-19e1-4606-9fc9-266f8d28b8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_sample_data(batch_size, max_seq_len, num_article_ids):\n",
    "    article_ids = torch.randint(1, num_article_ids, size=(batch_size, max_seq_len))\n",
    "    attention_mask = torch.ones((batch_size, max_seq_len))\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        num_pad = torch.randint(0, max_seq_len, size=(1,))\n",
    "        article_ids[i, -num_pad:] = 0\n",
    "        attention_mask[i, -num_pad:] = 0\n",
    "    \n",
    "    # Generate targets\n",
    "    max_purchase_article = 20\n",
    "    targets = torch.zeros((batch_size, num_article_ids))\n",
    "    for i in range(batch_size):\n",
    "        num_purchase_article = torch.randint(1, 20, size=(1,))\n",
    "        purchase_article_ids = torch.randint(1, num_article_ids, size=(num_purchase_article,))\n",
    "        targets[i, purchase_article_ids] = 1\n",
    "    \n",
    "    return {\"input_ids\": article_ids, \"attention_mask\": attention_mask}, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f583a4ad-1aa5-475d-b843-fb14da5d2be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: price, sales_channel_id などの情報を一緒に学習することができない\n",
    "x, targets = gen_sample_data(batch_size=8, max_seq_len=max_seq_len, num_article_ids=num_article_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8764af4b-1a06-4cb8-be26-15ef26ef26fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 93, 686, 973,  ...,   0,   0,   0],\n",
       "         [187, 309, 367,  ...,   0,   0,   0],\n",
       "         [329, 366, 601,  ...,   0,   0,   0],\n",
       "         ...,\n",
       "         [895, 114,  10,  ...,   0,   0,   0],\n",
       "         [125, 445, 503,  ...,   0,   0,   0],\n",
       "         [261, 625, 593,  ...,   0,   0,   0]]),\n",
       " 'attention_mask': tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
       "         [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "         [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "         [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "         [1., 1., 1.,  ..., 0., 0., 0.]])}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "df4b0dfd-e993-4e2e-ac70-67494bf84df0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1000])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca13bb2-d163-4e9e-8791-13f53eab7049",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "45341b05-347a-4e36-883b-01d25708392e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/transformers/v4.17.0/en/model_doc/bert#transformers.BertConfig\n",
    "config = transformers.RobertaConfig(\n",
    "    vocab_size=num_article_ids,\n",
    "    hidden_size=64,\n",
    "    num_hidden_layers=4,\n",
    "    num_attention_heads=4,\n",
    "    hidden_act=\"gelu\",\n",
    "    initializer_range=0.01,\n",
    "    layer_norm_eps=0.03,\n",
    "    dropout=0.3,\n",
    "    pad_token_id=0,\n",
    "    output_attentions=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b559788a-fdb1-4e6d-99c3-b608486a76e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaConfig {\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"dropout\": 0.3,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 64,\n",
       "  \"initializer_range\": 0.01,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 0.03,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"roberta\",\n",
       "  \"num_attention_heads\": 4,\n",
       "  \"num_hidden_layers\": 4,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.15.0\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 1000\n",
       "}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d980dbe7-af00-4e39-a2d0-92d3623952f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NextArticlePredictionHead(nn.Module):\n",
    "    \"\"\"\n",
    "    memo: transformers4rec では最終層をLogSoftmaxにしている\n",
    "    https://github.com/NVIDIA-Merlin/Transformers4Rec/blob/80596e89977c24736ed5ff22b6fef43fdd6a02f9/transformers4rec/torch/model/prediction_task.py#L321-L387\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        transformer_hidden_size=64,\n",
    "        output_size=num_article_ids,\n",
    "    ):\n",
    "        super(NextArticlePredictionHead, self).__init__()\n",
    "        self.hidden_size = transformer_hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.module = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size, output_size),\n",
    "            # torch.nn.LogSoftmax(dim=1),\n",
    "            # nn.Softmax(dim=1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.module(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d4418158-2325-4333-9e2a-5c70720ac9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, transformers_config):\n",
    "        super(Model, self).__init__()\n",
    "        self.transformer_model = transformers.RobertaModel(transformers_config)\n",
    "        self.head = NextArticlePredictionHead()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        model_outputs = self.transformer_model(**x)\n",
    "        outputs = model_outputs.last_hidden_state[:, 0, :]\n",
    "        x = self.head(outputs)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "535d48ac-8e74-414a-b729-4d1268b6242a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(transformers_config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1b4c984a-40a1-475e-ae56-b43b279275da",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "668e0560-0964-41f6-817c-2ddbe8ec1827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.BCELoss()\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e7bf406c-ff0d-4216-8518-bb9f973e74e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7297, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = criterion(pred, targets)\n",
    "loss.backward()\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd7199e-0620-48da-97a0-7772562e5dec",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "40ad5023-11e0-418e-9ff7-0951fe2c6417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/c/h-and-m-personalized-fashion-recommendations/discussion/307041\n",
    "# https://www.kaggle.com/kaerunantoka/h-m-how-to-calculate-map-12\n",
    "\n",
    "def average_precision(target, predict, k=12):\n",
    "    len_target = min(target.size(0), k)\n",
    "    \n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "    for i,p in enumerate(predict):\n",
    "        if p in target and p not in predict[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i+1.0)\n",
    "\n",
    "    return score / min(len_target, k)\n",
    "\n",
    "    \n",
    "def mean_average_precision(targets, predicts, k=12):\n",
    "    map_top_k = np.mean([average_precision(t, p) for t, p in zip(targets, predicts)])\n",
    "    assert 0.0 <= map_top_k <= 1.0, \"map_top_k must be 0.0 <= map_top_k <= 1.0\"\n",
    "    return map_top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "37a42851-f9fc-4451-850e-d5716b593a63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00224905303030303"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_article_ids = torch.topk(pred, 12)[1]\n",
    "target_ids = [t.nonzero().view(-1) for t in targets]\n",
    "\n",
    "mean_average_precision(target_ids, pred_article_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e7b7ef40-e852-492b-b119-eff04ce35dd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([ 15,  71, 208, 261, 295, 300, 347, 352, 500, 530, 594, 653, 684, 712,\n",
       "         717, 750, 755, 822]),\n",
       " tensor([  8,  14,  73,  78, 135, 147, 205, 233, 258, 361, 395, 534, 626, 642,\n",
       "         770, 789, 889, 911]),\n",
       " tensor([232, 444, 710, 712, 819, 881, 925, 935]),\n",
       " tensor([ 88, 120, 169, 342, 348, 356, 366, 432, 497, 539, 575, 592, 720, 750,\n",
       "         790, 806, 860, 867, 990]),\n",
       " tensor([661, 801, 931, 983]),\n",
       " tensor([ 23,  61, 103, 226, 237, 281, 335, 471, 489, 581, 606, 661, 675, 776,\n",
       "         806, 881, 915, 949, 951]),\n",
       " tensor([ 30,  78, 261, 314, 374, 380, 599, 645, 648, 723, 732, 734, 776]),\n",
       " tensor([ 29,  61, 121, 148, 161, 191, 242, 300, 424, 513, 549, 619, 657, 676])]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "785e6a5d-844f-4c03-98e2-89644a952c28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[562, 749, 264, 688, 916, 547, 844, 295,  82, 382, 772, 155],\n",
       "        [164, 234, 295,  70, 356,  72, 922,  82, 641, 132, 264, 833],\n",
       "        [288, 802, 560, 978, 799,  70, 442, 424, 583, 164,  72, 641],\n",
       "        [295, 375, 264,  72, 560, 288, 234, 831, 677, 191, 821, 620],\n",
       "        [288, 295, 560, 438, 833, 329, 495, 620, 234, 141, 164, 777],\n",
       "        [911, 764, 295,  80, 443, 164, 846, 234, 271, 198, 749, 264],\n",
       "        [254,  70, 234, 562, 155,  97, 295, 164, 189, 638, 599, 560],\n",
       "        [295, 198, 254, 777, 599, 155, 416,  82, 900, 459, 846, 292]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_article_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d1222c02-db3f-4d1e-80cc-b86e747e8f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: 完全一致のテスト\n",
    "tmp = torch.zeros((8, 12))\n",
    "\n",
    "for i, t in enumerate(target_ids):\n",
    "    tmp[i, :len(t)] = t[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1e7e19cf-e76e-4d7d-b0d9-45000e5c85d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_average_precision(target_ids, tmp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
